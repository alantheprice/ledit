//go:build ollama_test
// +build ollama_test

package security_validator

import (
	"context"
	"testing"
	"time"

	"github.com/alantheprice/ledit/pkg/configuration"
	"github.com/alantheprice/ledit/pkg/utils"
)

// TestRealOllamaValidation tests with actual Ollama model
func TestRealOllamaValidation(t *testing.T) {
	// Skip if running in CI or if Ollama is not available
	if testing.Short() {
		t.Skip("Skipping real Ollama test in short mode")
	}

	// Create validator with Ollama
	cfg := &configuration.SecurityValidationConfig{
		Enabled:        true,
		Model:          "qwen2.5-coder:0.5b",
		Threshold:      1,
		TimeoutSeconds: 30,
	}

	logger := utils.GetLogger(true)

	validator, err := NewOllamaValidator(cfg, logger, false)
	if err != nil {
		t.Fatalf("Failed to create Ollama validator: %v", err)
	}

	if validator == nil {
		t.Fatal("Validator is nil")
	}

	ctx := context.Background()

	// Test 1: Safe operation (read_file)
	t.Run("SafeOperation_ReadFile", func(t *testing.T) {
		start := time.Now()
		result, err := validator.ValidateToolCall(ctx, "read_file", map[string]interface{}{
			"file_path": "main.go",
		})
		latency := time.Since(start)

		if err != nil {
			t.Fatalf("ValidateToolCall failed: %v", err)
		}

		t.Logf("Read file validation:")
		t.Logf("  Risk Level: %s", result.RiskLevel)
		t.Logf("  Reasoning: %s", result.Reasoning)
		t.Logf("  Confidence: %.2f", result.Confidence)
		t.Logf("  Latency: %dms", result.LatencyMs)
		t.Logf("  Total Time: %dms", latency.Milliseconds())

		// Verify result structure
		if result.RiskLevel < RiskSafe || result.RiskLevel > RiskDangerous {
			t.Errorf("Invalid risk level: %d", result.RiskLevel)
		}

		if result.ModelUsed == "" {
			t.Error("ModelUsed should not be empty")
		}

		if result.LatencyMs < 0 {
			t.Error("LatencyMs should be non-negative")
		}

		// Log actual latency for analysis
		if result.LatencyMs > 5000 {
			t.Logf("WARNING: High latency detected: %dms", result.LatencyMs)
		}
	})

	// Test 2: Caution operation (shell command with git reset)
	t.Run("CautionOperation_GitReset", func(t *testing.T) {
		start := time.Now()
		result, err := validator.ValidateToolCall(ctx, "shell_command", map[string]interface{}{
			"command": "git reset --hard HEAD",
		})
		latency := time.Since(start)

		if err != nil {
			t.Fatalf("ValidateToolCall failed: %v", err)
		}

		t.Logf("Git reset validation:")
		t.Logf("  Risk Level: %s", result.RiskLevel)
		t.Logf("  Reasoning: %s", result.Reasoning)
		t.Logf("  Confidence: %.2f", result.Confidence)
		t.Logf("  Latency: %dms", result.LatencyMs)
		t.Logf("  Total Time: %dms", latency.Milliseconds())

		// This should be at least CAUTION level
		if result.RiskLevel < RiskCaution {
			t.Logf("WARNING: Expected CAUTION or DANGEROUS for git reset, got %s", result.RiskLevel)
		}
	})

	// Test 3: Dangerous operation (recursive deletion)
	t.Run("DangerousOperation_RmRf", func(t *testing.T) {
		start := time.Now()
		result, err := validator.ValidateToolCall(ctx, "shell_command", map[string]interface{}{
			"command": "rm -rf /tmp/test",
		})
		latency := time.Since(start)

		if err != nil {
			t.Fatalf("ValidateToolCall failed: %v", err)
		}

		t.Logf("Recursive deletion validation:")
		t.Logf("  Risk Level: %s", result.RiskLevel)
		t.Logf("  Reasoning: %s", result.Reasoning)
		t.Logf("  Confidence: %.2f", result.Confidence)
		t.Logf("  Latency: %dms", result.LatencyMs)
		t.Logf("  Total Time: %dms", latency.Milliseconds())

		// This should be DANGEROUS
		if result.RiskLevel != RiskDangerous {
			t.Logf("WARNING: Expected DANGEROUS for rm -rf, got %s", result.RiskLevel)
			t.Logf("This might indicate the model is being too permissive")
		}
	})

	// Test 4: Context-aware evaluation
	t.Run("ContextAware_LsVsRm", func(t *testing.T) {
		// Test ls (should be safe)
		lsResult, err := validator.ValidateToolCall(ctx, "shell_command", map[string]interface{}{
			"command": "ls -la",
		})
		if err != nil {
			t.Fatalf("ValidateToolCall failed for ls: %v", err)
		}

		// Test rm (should be more dangerous)
		rmResult, err := validator.ValidateToolCall(ctx, "shell_command", map[string]interface{}{
			"command": "rm file.txt",
		})
		if err != nil {
			t.Fatalf("ValidateToolCall failed for rm: %v", err)
		}

		t.Logf("Context-aware comparison:")
		t.Logf("  ls -la risk: %s", lsResult.RiskLevel)
		t.Logf("  rm file.txt risk: %s", rmResult.RiskLevel)

		// rm should be riskier than ls
		if rmResult.RiskLevel <= lsResult.RiskLevel {
			t.Logf("WARNING: rm (%s) should be riskier than ls (%s)", rmResult.RiskLevel, lsResult.RiskLevel)
		}
	})

	// Test 5: Threshold application
	t.Run("ThresholdApplication", func(t *testing.T) {
		// Test with threshold 0 (permissive)
		cfgPermissive := &configuration.SecurityValidationConfig{
			Enabled:        true,
			Model:          "qwen2.5-coder:0.5b",
			Threshold:      0,
			TimeoutSeconds: 30,
		}

		validatorPermissive, err := NewOllamaValidator(cfgPermissive, logger, false)
		if err != nil {
			t.Fatalf("Failed to create permissive validator: %v", err)
		}

		result, err := validatorPermissive.ValidateToolCall(ctx, "shell_command", map[string]interface{}{
			"command": "rm test.txt",
		})
		if err != nil {
			t.Fatalf("ValidateToolCall failed: %v", err)
		}

		t.Logf("Threshold 0 test:")
		t.Logf("  Risk: %s, ShouldConfirm: %v", result.RiskLevel, result.ShouldConfirm)

		// With threshold 0, even risky operations should not request confirmation
		if result.ShouldConfirm {
			t.Logf("NOTE: With threshold 0, ShouldConfirm should be false, got true")
		}
	})
}

// TestOllamaLatency benchmarks actual Ollama latency
func TestOllamaLatency(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping latency test in short mode")
	}

	cfg := &configuration.SecurityValidationConfig{
		Enabled:        true,
		Model:          "qwen2.5-coder:0.5b",
		Threshold:      1,
		TimeoutSeconds: 30,
	}

	logger := utils.GetLogger(true)

	validator, err := NewOllamaValidator(cfg, logger, false)
	if err != nil {
		t.Fatalf("Failed to create Ollama validator: %v", err)
	}

	ctx := context.Background()

	// Test a realistic mix of operations
	testCases := []struct {
		name     string
		toolName string
		args     map[string]interface{}
	}{
		{"read_file (pre-filtered)", "read_file", map[string]interface{}{"file_path": "test.go"}},
		{"ls -la (pre-filtered)", "shell_command", map[string]interface{}{"command": "ls -la"}},
		{"rm file.txt (needs LLM)", "shell_command", map[string]interface{}{"command": "rm file.txt"}},
		{"rm -rf /tmp/test (needs LLM)", "shell_command", map[string]interface{}{"command": "rm -rf /tmp/test"}},
	}

	totalLatency := 0
	totalLLMLatency := int64(0)
	preFilteredCount := 0

	for _, tc := range testCases {
		start := time.Now()
		result, err := validator.ValidateToolCall(ctx, tc.toolName, tc.args)
		latency := time.Since(start)

		if err != nil {
			t.Fatalf("Validation %s failed: %v", tc.name, err)
		}

		totalLatency += int(latency.Milliseconds())
		totalLLMLatency += result.LatencyMs

		if result.ModelUsed == "prefilter" {
			preFilteredCount++
		}

		t.Logf("%s:", tc.name)
		t.Logf("  Total Latency: %dms", latency.Milliseconds())
		t.Logf("  LLM Latency: %dms", result.LatencyMs)
		t.Logf("  Risk: %s", result.RiskLevel)
		t.Logf("  Model: %s", result.ModelUsed)
	}

	avgLatency := totalLatency / len(testCases)
	avgLLMLatency := int(totalLLMLatency) / (len(testCases) - preFilteredCount)

	t.Logf("\nLatency Summary:")
	t.Logf("  Average Total: %dms", avgLatency)
	t.Logf("  Pre-filtered: %d/%d operations (%.0f%%)", preFilteredCount, len(testCases),
		float64(preFilteredCount)/float64(len(testCases))*100)
	t.Logf("  Average LLM Latency (for non-pre-filtered): %dms", avgLLMLatency)
	t.Logf("  Target: <100ms for acceptable performance")

	if avgLatency > 1000 {
		t.Logf("WARNING: Average latency %dms exceeds 1 second target", avgLatency)
	}

	if preFilteredCount > 0 {
		t.Logf("âœ“ Pre-filtering is working - %d operations skipped LLM validation", preFilteredCount)
	}
}

// TestOllamaModelNotAvailable tests behavior when model isn't available
func TestOllamaModelNotAvailable(t *testing.T) {
	// Test with a model that doesn't exist
	cfg := &configuration.SecurityValidationConfig{
		Enabled:        true,
		Model:          "nonexistent-model:latest",
		Threshold:      1,
		TimeoutSeconds: 5,
	}

	logger := utils.GetLogger(true)

	validator, err := NewOllamaValidator(cfg, logger, false)
	if err != nil {
		// Expected to fail when creating validator
		t.Logf("Expected failure when model doesn't exist: %v", err)
		return
	}

	// If it somehow succeeded, test that validation fails gracefully
	ctx := context.Background()
	result, err := validator.ValidateToolCall(ctx, "read_file", map[string]interface{}{
		"file_path": "test.go",
	})

	if err != nil {
		t.Logf("Validation failed as expected: %v", err)
	}

	if result != nil && result.RiskLevel == RiskCaution {
		t.Logf("Correctly defaulted to CAUTION when model unavailable")
	}
}
